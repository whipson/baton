---
title: "Create maestro pipelines"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{maestro-create-pipelines}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Overview
A data pipeline is a crucial concept for anyone working with data, from analysts to seasoned data scientists. At its core, a data pipeline is a series of steps designed to efficiently and systematically handle and transform data from its raw form into usable information. Think of a data pipeline as a factory assembly line where data is the raw material. As this data travels along the pipeline, it undergoes various transformations—such as cleaning, aggregation, and analysis—making it increasingly refined and valuable.

An essential aspect of data pipelines is scheduling. Automated schedules ensure data processes are reliable and executed in a timely manner, without requiring manual intervention. This is key for scalability, as it allows systems to handle increasing amounts of data efficiently. Scheduling also supports consistent automation, minimizing errors and maximizing the productivity.

`maestro` allows R users to write data pipeline using conventional R methods and syntax that can be automated via a scheduled orcherstrator.

<br>

## Decorators
A decorator is a special tag that precedes the pipeline script that indicates how they should be interpreted and processed. `maestro` uses decorators to identify how to schedule the pipeline with the orchestrator. Below are the main decorators that are used to build a schedule for use within the orchestrator.

<br>

### Maestro Frequency
The `@maestroFrequency` tag identifies when the pipeline runs. `@maestroFrequency` requires one of the following parameters `minute, hour, day, week, month, quarter, year`.

An example of using this tag is `@maestroFrequency day`

<br>

### Maestro Interval
The `@maestroInterval` tag works in conjunction with `@maestroFrequency` to indicate how often the pipeline is to run. `@maestroInterval` requires a whole number as a parameter.

An example of using this tag is `@maestroInterval 1`. `@maestroInterval 1` combined with `@maestroFrequency day` would run the pipeline everyday, where as `@maestroInterval 3` combined with `@maestroFrequency day` would run the pipeline every three days.

<br>

### Maestro Start Time
The `@maestroStartTime` tag identifies when the pipeline runs for the first time. The tag is also used in conjunction with `@maestroFrequency` and `@maestroInterval` to calculate when the pipeline is to run next. `@maestroStartTime` requires a datetime parameter using the `yyyy-mm-dd hh:mm:ss` format.

An example of using this tag is `@maestroStartTime 2024-04-25 05:45:00`

<br>

### Maestro Timezone
The `@maestroTz` tag identifies what timezone is to be used when scheduling the pipeline. `@maestroTz` requires a single parameter based on timezones from the `OlsonNames` R function.

An example of using this tag is `@maestroTz America/Halifax`

<br>

### Maestro Skip
The `@maestroSkip` tag provides the option for a pipeline to be ignored in the creation of the orchestration schedule, which is helpful for pipelines that are in development. `@maestroSkip` can be added to the script without any parameter values.

An example of using this tag is `@maestroSkip`

<br>


# Pipelines Examples
A `maestro` pipeline consists of decorators used to schedule the pipeline, and tasks/steps of R code written in a single function, saved as a R script. The `maestro::new_pipeline()` function creates a template pipeline script within the project `pipelines` folder. The function can be used in one of two ways:

  1) providing a pipeline name and leveraging the default settings
  2) providing a pipeline name and expressively declaring the pipeline variables
  
The following are examples of each approach.

<br>

**Leveraging the default settings**

`maestro::new_pipeline("pipeline_climate_daily")`

```{r eval = FALSE, message = FALSE, warning = FALSE}
#' pipeline_climate_daily maestro pipeline
#'
#' @maestroFrequency 
#' @maestroInterval 
#' @maestroStartTime 
#' @maestroTz 

pipeline_climate_daily <- function() {

  # Pipeline code
}
```

</br>
</br>

**Expressively declaring the pipeline variables**

```
maestro::new_pipeline(
  pipe_name = "pipeline_climate_daily",
  pipeline_dir = "pipelines",
  frequency = "day",
  interval = 1,
  start_time = "2024-04-25 06:30:00",
  tz = "America/Halifax",
  open = TRUE
  )
```

```{r eval = FALSE, message = FALSE, warning = FALSE}
#' pipeline_climate_daily maestro pipeline
#'
#' @maestroFrequency day
#' @maestroInterval 1
#' @maestroStartTime 2024-04-25 06:30:00
#' @maestroTz America/Halifax

pipeline_climate_daily <- function() {

  # Pipeline code
}
```

<br>

## Hourly Pipeline
This pipeline is an example of a standard *extract transform load* (ETL) workflow. The pipeline is scheduled to run every **3 hours** starting on **2024-04-25** at **05:45:00**. The goal of the pipeline is to perform the following:

  - access online hosted CSV file
  - perform lite data wrangling
  - write file to local storage in parquet format

<br>

This example is setup as a simple set of tasks creating objects that are used in the next series of tasks. All components of the pipeline are within the `pipeline_wildfire_hourly` function, which has no parameters.

<br>

```{r eval = FALSE, message = FALSE, warning = FALSE}
#' pipeline_wildfire_hourly maestro pipeline
#'
#' @maestroFrequency hour
#' @maestroInterval 3
#' @maestroStartTime 2024-04-25 05:45:00
#' @maestroTz America/Halifax


pipeline_wildfire_hourly <- function() {
  
  # load libraries
  library(dplyr)
  library(readr)
  library(sf)
  library(sfarrow)

  # Access active wildfire data from hosted csv
  df <- readr::read_csv("https://cwfis.cfs.nrcan.gc.ca/downloads/activefires/activefires.csv")
  
  # Data wrangling
  df_geom <- df |>
    dplyr::mutate(insert_datetime = Sys.time()) |>
    sf::st_as_sf(coords = c("lon", "lat"), crs = 4326)
  
  
  # Write active wildfires to file
  basename <- paste("cdn_wildfire", as.integer(Sys.time()), sep = "_")
  
  df_geom |>
    sfarrow::write_sf_dataset("~/data/wildfires",
                              format = "parquet",
                              basename_template = paste0(basename,
                                                         "-{i}.parquet"))
}
```

</br>

## Daily Pipeline
This pipeline is an example of a standard *extract transform load* (ETL) workflow. The pipeline is scheduled to run every **day** starting on **2024-04-25** at **06:30:00**. The goal of the pipeline is to perform the following:

  - submit a request to an API
  - extract data from the API
  - add insert datetime column
  - write file to local storage in parquet format

<br>

This example has a custom function that is used to access and extract the data from the API, which is piped into additional tasks. All components of the pipeline are within the `pipeline_climate_daily` function, which has no parameters.

<br>

```{r eval = FALSE, message = FALSE, warning = FALSE}
#' pipeline_climate_daily maestro pipeline
#'
#' @maestroFrequency day
#' @maestroInterval 1
#' @maestroStartTime 2024-04-25 06:30:00
#' @maestroTz America/Halifax

pipeline_climate_daily <- function() {
  
  # load libraries
  library(dplyr)
  library(httr2)
  library(arrow)
  
  # Custom function for accessing api climate data
  get_hourly_climate_info <- function(station_id, request_limit = 24) {
  
  # Validate parameters
  stopifnot("`station_id` must be a real number" = is.numeric(station_id) && station_id > 0)
  stopifnot("`station_id` must be a length-one vector" = length(station_id) == 1)
  
  # Access climate hourly via geomet api 
  hourly_req <- httr2::request("https://api.weather.gc.ca/collections/climate-hourly/items") |> 
    httr2::req_url_query(
      lang = "en-CA",
      offset = 0,
      CLIMATE_IDENTIFIER = station_id,
      LOCAL_DATE = Sys.Date() - 1,
      limit = request_limit
    )
  
  # Perform the request
  hourly_resp <- hourly_req |> 
    httr2::req_perform()
  
  # Climate station response to data frame
  geomet_json <- hourly_resp |> 
    httr2::resp_body_json(simplifyVector = TRUE)
  
  geomet_json$features
  }
  
  # Write climate hourly to file
  basename <- paste("climate_hourly", as.integer(Sys.time()), sep = "_")

  get_hourly_climate_info(8202251) |>
    dplyr::mutate(insert_datetime = Sys.time()) |>
    arrow::write_dataset(
      "~/data/climate",
      format = "parquet",
      basename_template = basename
      )
}

```






